{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d722d6d-c76c-45be-b125-9bd9cfd8d429",
   "metadata": {},
   "source": [
    "# Milking the final drops of Accuracy\n",
    "\n",
    "So... you've trained a model.  \n",
    "\n",
    "Now what? You could squeeze a few more epochs, but that'll only lead to small, negligible gains.  \n",
    "\n",
    "Or you could really scrape the bottom of the barrel and try the techniques in this notebook...? We'll try to go through some methods (e.g. SWA, KD) to get better results from our model. We use the model from the Speedrunning Model Training notebook, which itself is undertrained. Thus, this notebook will come in very handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f76f85d5-a258-41d8-a952-1c7ecdc93bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision.models import resnet18\n",
    "import pytorch_lightning as pl\n",
    "import torchmetrics\n",
    "from torchvision.transforms import v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0f8f5705-13ae-473f-ae97-e53e3a274c0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((32, 32)),\n",
    "    v2.AutoAugment(v2.AutoAugmentPolicy.CIFAR10),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize([0.48, 0.44, 0.40], [0.22, 0.22, 0.22])\n",
    "])\n",
    "test_transforms = v2.Compose([\n",
    "    v2.ToImage(),\n",
    "    v2.Resize((32, 32)),\n",
    "    v2.ToDtype(torch.float32, scale=True),\n",
    "    v2.Normalize([0.48, 0.44, 0.40], [0.22, 0.22, 0.22])\n",
    "])\n",
    "\n",
    "train_ds = torchvision.datasets.CIFAR10(root=\"data\", train=True, download=True, transform=train_transforms)\n",
    "test_ds = torchvision.datasets.CIFAR10(root=\"data\", train=False, download=True, transform=test_transforms)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=3, prefetch_factor=1, persistent_workers=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False, num_workers=3, prefetch_factor=1, persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a2f84e-3275-4612-b640-e9b988dcbaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.set_float32_matmul_precision('high')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc834d02-9e9d-4df6-a294-4708c11ec8ed",
   "metadata": {},
   "source": [
    "Our model is just a ResNet18 trained on CIFAR-10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c0fd129-cf86-4323-9e43-e22675bd2c57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = resnet18()\n",
    "model.fc = nn.Linear(512, 10)\n",
    "model.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004a5f08-530b-473d-afc5-3d22d14a0179",
   "metadata": {},
   "source": [
    "## 1. Stochastic Weight Averaging (SWA)\n",
    "SWA is a method where you average the weights of a model over several training iterations. This way, you get a more generalized model, which should hopefully perform better.  \n",
    "\n",
    "Both PyTorch and PyTorch Lightning have in-built methods for performing SWA. I'm a big fan of being lazy, so let's use Lightning. Below is a simple template class that is responsible for handling the training steps and calculate metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a2749bd9-e354-4305-ba6a-034f19386206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LitSWAResNet(pl.LightningModule):\n",
    "    def __init__(self, resnet_cls, lr):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_cls()\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=10)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.resnet(X)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        self.log(\"train_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.resnet(X)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.resnet.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "model = LitSWAResNet(resnet18, 2e-5) # Our min_lr from the previous notebook - go check it out!\n",
    "model.resnet.load_state_dict(torch.load(\"model.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba2330-ccf1-4cf4-b694-0a7b81660c1d",
   "metadata": {},
   "source": [
    "In PyTorch Lightning, the `StochasticWeightAveraging` callback manages SWA. It accepts a `swa_lrs`, or the SWA Learning Rate. It's typically moderately high, such as `1e-3` in this case, as it has to explore the areas around the current local minima.  \n",
    "\n",
    "We average the weights over 5 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cae02f68-3ced-4400-b2bd-9cb7f6a4ea68",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | resnet   | ResNet             | 11.2 M | train\n",
      "1 | accuracy | MulticlassAccuracy | 0      | train\n",
      "--------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.727    Total estimated model params size (MB)\n",
      "69        Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "250615243a8d4ffbb8bd8a15ad2b5dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f643ddc40174005984e4adc2ea82171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "81b30c40b3564702bd44ce0842c69b60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9737e3633d534234bc4facef70ca5e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "402f7cc406f540a79c6531eb3fb70d7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1feaf531ba0e4a4d8a5740604734ff1f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "669cac543af14a3bb81106c2779afe04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb23f7f415fa474db60c902fe780698d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=6` reached.\n"
     ]
    }
   ],
   "source": [
    "swa_callback = pl.callbacks.StochasticWeightAveraging(swa_lrs=1e-3, swa_epoch_start=0.0)\n",
    "trainer = pl.Trainer(precision=\"bf16-mixed\", max_epochs=5, check_val_every_n_epoch=1, log_every_n_steps=5, callbacks=[swa_callback])\n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "13e087c4-7042-4249-be21-3ffe01fe9096",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45cf2e23191b4f8ba2eb4700e1e3d555",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8374999761581421     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">     0.608993411064148     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8374999761581421    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m    0.608993411064148    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.608993411064148, 'val_acc': 0.8374999761581421}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9727a5d-bf5d-4be0-9184-606d1cc88cd9",
   "metadata": {},
   "source": [
    "Alright... a negligible accuracy increase. To be fair, it's all within the margin of error. Besides, SWA is known to give only incremental improvements!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27d5095-dcdf-4627-8b8d-f01e121c2e73",
   "metadata": {},
   "source": [
    "## 2: Self-Distillation\n",
    "If you've heard of *knowledge distillation* this should be easier to grasp. If you haven't, knowledge distillation (KD) is where a smaller model (called a *student*) is trained to mimic the outputs of a larger, trained model (called a *teacher*). This is because the larger model's outputs will not be discrete and fixed like the target objective, but rather *soft targets* that shift with the input data, and provide subtle biases in the non-target distributions that may guide the student model in training.  \n",
    "\n",
    "For example, when classifying a truck, the target objective may have a `1` for the truck class, and `0` everywhere else. But a teacher model may put some weight on classes like bus, car, lorry - which helps the model learn more quickly the associations between classes.  \n",
    "\n",
    "Self-distillation, then is a *special type of knowledge distillation*. Instead of distilling the knowledge into a smaller model, you distill into a model of the **same size**. Papers suggest that this is because providing a teacher model leads to smoother gradients towards better minima - check out https://arxiv.org/pdf/2206.08491 for a rather enjoyable read on this topic!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cc9fadb-a6ce-42ae-aa69-4e6f70da56ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher_model = model.resnet\n",
    "for param in teacher_model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74ddcae5-764d-4ced-a3cc-c8a142a56eed",
   "metadata": {},
   "source": [
    "We take the class we defined for training the teacher model (from the previous notebook) and modify it slightly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "55738627-f722-4112-a830-310d79d0acc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSelfDistillStudent(pl.LightningModule):\n",
    "    def __init__(self, resnet_cls, teacher_model, temperature, max_lr, min_lr, epochs, wd, kd_balance=0.5):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_cls()\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.max_lr = max_lr\n",
    "        self.min_lr = min_lr\n",
    "        self.epochs = epochs\n",
    "        self.wd = wd\n",
    "\n",
    "        self.accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=10)\n",
    "        \n",
    "        self.apply(self._init_weights)\n",
    "        nn.init.xavier_uniform_(self.resnet.fc.weight)\n",
    "        nn.init.zeros_(self.resnet.fc.bias)\n",
    "\n",
    "        #######################################\n",
    "        # NEW! Add self-distillation parameters\n",
    "        self.teacher = teacher_model\n",
    "        self.temperature = temperature\n",
    "        self.kd_balance = kd_balance\n",
    "        #######################################\n",
    "        \n",
    "    def _init_weights(self, m):\n",
    "        if isinstance(m, nn.Conv2d):\n",
    "            nn.init.kaiming_normal_(m.weight, nonlinearity=\"relu\", mode=\"fan_out\")\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "        elif isinstance(m, nn.BatchNorm2d):\n",
    "            nn.init.ones_(m.weight)\n",
    "            if m.bias is not None:\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        \n",
    "        #########################\n",
    "        # NEW: Distillation logic\n",
    "        with torch.inference_mode():\n",
    "            teacher_logits = self.teacher(X)\n",
    "        \n",
    "        student_logits = self.resnet(X)\n",
    "\n",
    "        # We soften everything by dividing by temperature so that the targets arent so sharp, and easier to learn\n",
    "        # Also log_softmax for KL divergence for numerical stability\n",
    "        soft_teacher_targets = nn.functional.log_softmax(teacher_logits / self.temperature, dim=-1)\n",
    "        soft_student_probs = nn.functional.log_softmax(student_logits / self.temperature, dim=-1)\n",
    "        \n",
    "        labels_loss = nn.functional.cross_entropy(student_logits, y)\n",
    "        self.log(\"train_labels_loss\", labels_loss)\n",
    "        teacher_loss = nn.functional.kl_div(soft_student_probs, soft_teacher_targets, log_target=True) * (self.temperature**2) # scaled according to original distillation paper\n",
    "        self.log(\"train_teacher_loss\", teacher_loss)\n",
    "        loss = self.kd_balance*labels_loss + (1-self.kd_balance)*teacher_loss\n",
    "        #########################\n",
    "        \n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = self.accuracy(student_logits, y)\n",
    "        self.log(\"train_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.resnet(X)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.resnet.parameters(), lr=self.min_lr, weight_decay=self.wd)\n",
    "        scheduler = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr=self.max_lr, epochs=self.epochs, steps_per_epoch=len(train_loader), div_factor=self.max_lr/self.min_lr)\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "75014bf2-f647-4de9-a741-cb4474b157ce",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using bfloat16 Automatic Mixed Precision (AMP)\n",
      "Using default `ModelCheckpoint`. Consider installing `litmodels` package to enable `LitModelCheckpoint` for automatic upload to the Lightning model registry.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name     | Type               | Params | Mode \n",
      "--------------------------------------------------------\n",
      "0 | resnet   | ResNet             | 11.2 M | train\n",
      "1 | accuracy | MulticlassAccuracy | 0      | train\n",
      "2 | teacher  | ResNet             | 11.2 M | train\n",
      "--------------------------------------------------------\n",
      "11.2 M    Trainable params\n",
      "11.2 M    Non-trainable params\n",
      "22.4 M    Total params\n",
      "89.453    Total estimated model params size (MB)\n",
      "137       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5bff6e1da104d849c7be88eedca7432",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e4e051fdb7f4a38987aceeac2cdd659",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0620 07:18:11.961000 17128 torch/_inductor/utils.py:1250] [5/0_1] Not enough SMs to use max_autotune_gemm mode\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "W0620 07:18:19.905000 17128 torch/_dynamo/convert_frame.py:964] [1/8] torch._dynamo hit config.recompile_limit (8)\n",
      "W0620 07:18:19.905000 17128 torch/_dynamo/convert_frame.py:964] [1/8]    function: 'log' (/venv/main/lib/python3.12/site-packages/pytorch_lightning/core/module.py:376)\n",
      "W0620 07:18:19.905000 17128 torch/_dynamo/convert_frame.py:964] [1/8]    last reason: 1/7: name == 'train_acc'                                    \n",
      "W0620 07:18:19.905000 17128 torch/_dynamo/convert_frame.py:964] [1/8] To log all recompilation reasons, use TORCH_LOGS=\"recompiles\".\n",
      "W0620 07:18:19.905000 17128 torch/_dynamo/convert_frame.py:964] [1/8] To diagnose recompilation issues, see https://pytorch.org/docs/main/torch.compiler_troubleshooting.html.\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n",
      "/venv/main/lib/python3.12/site-packages/torch/nn/functional.py:3384: UserWarning: reduction: 'mean' divides the total loss by both the batch size and the support size.'batchmean' divides only by the batch size, and aligns with the KL div math definition.'mean' will be changed to behave the same as 'batchmean' in the next major release.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94df15428abf4c21b40396c4927a110f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "defa68e4cf02428782874502d96e8378",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ab85d2dfa984c0ea9fb20de6de728cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ba4447b3d4247679327000a6c65e18e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa1279b477641a88e2d458a4c8e3393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03cc78853da9449996bcc9e3b865ca5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bc9834e756ea4c6d9e7f04a4dcf516e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "268cbe81a72246b4b26279af33336673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd05b87cd580489c95c773420b252bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23b534d5e4cc4a6cbe76b1a72e1810a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f37dbc308134346a5664d8de85e5b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38baea2bc1124d1998614486b33f74e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc6d72a4f8af477f948cb988e0700ff7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d161ebd3ad2242d89dfc4edce2811aa6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "826a32c7507e49fdaf83ff7fd82bf741",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6024f338f9074db7b8825b0af7bf0f22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3783a9e2cd640e0a6a6c4ad1b4bb59a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24993f8c42ab46bbbcc6a1c80f73a850",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bacb4b762064da087a3c0fa50c850ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf0cc188d9104e31bcab2a872e726d24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 11min 24s, sys: 39.2 s, total: 12min 3s\n",
      "Wall time: 11min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = torch.compile(LSelfDistillStudent(resnet18, teacher_model, 2.0, 5e-3, 2e-5, 100, 1e-2))\n",
    "trainer = pl.Trainer(precision=\"bf16-mixed\", max_epochs=100, check_val_every_n_epoch=5, log_every_n_steps=5) \n",
    "trainer.fit(model, train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fbee81ed-b626-477f-8bbb-b23a7872f174",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8514219952946e19e58e40795722c4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8440999984741211     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.5827531218528748     </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8440999984741211    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5827531218528748    \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.5827531218528748, 'val_acc': 0.8440999984741211}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f8cb9f-3375-4857-aa3f-12ae9db734f6",
   "metadata": {},
   "source": [
    "And bam! +0.7% from the original result!  \n",
    "\n",
    "We're almost at the end - there's one last thing we can try that involves no training at all. (No we are not doing ensembling although you could definitely do that for better performance!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092eae14-79cd-410c-83fd-bb2d2ee7419f",
   "metadata": {},
   "source": [
    "## 3. Test-Time Augmentation\n",
    "By augmenting the images during test time and averaging the outputs, we can see if there is an increase in accuracy. Sounds counter-intuitive, but when the model has more chances to look at the same image from different angles, the model can make a better guess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e379d821-2256-4629-8bfd-c0f6c95ed810",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class LitSWAResNet(pl.LightningModule):\n",
    "    def __init__(self, resnet_cls, lr):\n",
    "        super().__init__()\n",
    "        self.resnet = resnet_cls()\n",
    "        self.resnet.fc = nn.Linear(512, 10)\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.accuracy = torchmetrics.Accuracy(\"multiclass\", num_classes=10)\n",
    "\n",
    "        ###################################\n",
    "        # NEW! Add test-time augmentations.\n",
    "        self.test_augs = [\n",
    "            None,\n",
    "            v2.RandomHorizontalFlip(p=1.0)\n",
    "        ]\n",
    "        ###################################\n",
    "        \n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        y_pred = self.resnet(X)\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"train_loss\", loss)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        self.log(\"train_acc\", acc)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        X, y = batch\n",
    "        #####################################\n",
    "        # NEW! Test-time augmentation itself.\n",
    "        preds = []\n",
    "        for aug in self.test_augs:\n",
    "            if aug is not None:\n",
    "                y_pred = self.resnet(aug(X))\n",
    "            else:\n",
    "                y_pred = self.resnet(X)\n",
    "            preds.append(y_pred)\n",
    "        y_pred = torch.mean(torch.stack(preds, dim=0), dim=0)\n",
    "        #####################################\n",
    "        loss = nn.functional.cross_entropy(y_pred, y)\n",
    "        self.log(\"val_loss\", loss)\n",
    "        acc = self.accuracy(y_pred, y)\n",
    "        self.log(\"val_acc\", acc)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.resnet.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "model = LitSWAResNet(resnet18, 2e-5)\n",
    "model.resnet.load_state_dict(torch.load(\"model_student.pth\", weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1350d6a5-bed6-4278-bad1-83c4957f2fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "228f762426fa4193b705c6c3c698ab9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\">      Validate metric      </span>┃<span style=\"font-weight: bold\">       DataLoader 0        </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">          val_acc          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.8666999936103821     </span>│\n",
       "│<span style=\"color: #008080; text-decoration-color: #008080\">         val_loss          </span>│<span style=\"color: #800080; text-decoration-color: #800080\">    0.45302221179008484    </span>│\n",
       "└───────────────────────────┴───────────────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1m     Validate metric     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
       "│\u001b[36m \u001b[0m\u001b[36m         val_acc         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8666999936103821    \u001b[0m\u001b[35m \u001b[0m│\n",
       "│\u001b[36m \u001b[0m\u001b[36m        val_loss         \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.45302221179008484   \u001b[0m\u001b[35m \u001b[0m│\n",
       "└───────────────────────────┴───────────────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 0.45302221179008484, 'val_acc': 0.8666999936103821}]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.validate(model, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99509744-4c68-40da-90f6-c3eaccbf99b0",
   "metadata": {},
   "source": [
    "Bam! Instant +2.2% improvement. One thing to note is what augmentations you use though - initially I used multiple (vertical flip, rotate 90 degrees) but I realized they were performing poorly because the model was not trained with those augmentations.  \n",
    "\n",
    "To be fair the entire model was trained with very weak augmentation. It could defintely perform better with those."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
